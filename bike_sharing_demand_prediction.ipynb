{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "3RnN4peoiCZX",
        "7hBIi_osiCS2",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "iky9q4vBYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makadnan/alma_projects/blob/main/bike_sharing_demand_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Regression on Bike Sharing Demand\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this machine learning project was to predict the demand for bike sharing using different regression models. The project involved cleaning and processing a large dataset of bike sharing data that included a range of features such as weather conditions, time, day of the week, holiday status, and more. The cleaned data was then used to train and test several regression models, including linear regression, decision tree regression, random forest regression, and gradient boosting regression.\n",
        "\n",
        "After evaluating the performance of each model, it was found that the gradient boosting regression model provided the most accurate predictions with a root mean squared error (RMSE) of 41.78. The model was able to predict the demand for bike sharing with an accuracy of approximately 87%, which is considered to be a good level of accuracy for this type of prediction task.\n",
        "\n",
        "The project results demonstrate the importance of feature selection and engineering in improving the accuracy of the regression models. By including relevant features such as weather conditions and holiday status, the models were able to better capture the factors that influence bike sharing demand.\n",
        "\n",
        "Overall, the project highlights the potential of machine learning techniques in predicting bike sharing demand and the importance of choosing the right regression model for the specific prediction task at hand. The findings of this project could be useful for bike sharing companies to better plan their operations and resources based on predicted demand. The project could also inspire further research into the use of machine learning for demand prediction in other domains.\n",
        "\n",
        "Future work could involve exploring other regression models or techniques, such as neural networks or support vector regression, to further improve the accuracy of the predictions. Additionally, it could be beneficial to include more granular data, such as data on individual stations and their specific characteristics, to improve the accuracy of the predictions even further.\n",
        "\n",
        "In conclusion, this machine learning project has demonstrated the effectiveness of various regression models in predicting bike sharing demand based on weather conditions, time, day of the week, and other relevant features. The gradient boosting regression model provided the most accurate predictions, with an RMSE of 41.78 and an accuracy of approximately 87%. The findings of this project could be useful for bike sharing companies in better planning their operations and resources, and could inspire further research into the use of machine learning for demand prediction in other domains."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This dataset consists of the number of public bikes rented in Seoul's bike sharing system at each hour. It also includes information about the weather and the time, such as whether it was a public holiday.So, this project is an attempt to develop a machine learning model to accurately predict the demand for bike rentals in a bike-sharing system based on historical usage patterns and external factors such as weather conditions, time of day, and holidays. The model should be able to forecast the number of bikes that will be rented at different times and locations, allowing the bike-sharing company to optimize its bike allocation and improve customer satisfaction.\"**\n",
        "\n",
        "We will look into the data and try to anser these questions such as:\n",
        "\n",
        "*   What are the trends and patterns in bike rental demand over time?\n",
        "*   How does demand vary by location and time of day?\n",
        "*   What external factors affect bike rental demand, such as weather conditions or holidays?\n",
        "*   How accurate can we predict bike rental demand based on historical data and external factors?\n",
        "*  How can we use machine learning to optimize bike allocation and improve customer satisfaction?"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing imporant libraries\n",
        "import pandas as pd                                         # for data manipulation and analysis\n",
        "import numpy as np                                          # for numerical operations\n",
        "import matplotlib.pyplot as plt                             # for data visualization\n",
        "import seaborn as sns                                       # for high level visualization\n",
        "from pandas.plotting import scatter_matrix                  #for scatter plots of mutiple variables\n",
        "from scipy.stats import norm, stats, boxcox                 #for log transformation of columns\n",
        "from sklearn.preprocessing import StandardScaler            #for feature scaling\n",
        "from sklearn.model_selection import StratifiedShuffleSplit  #proportionally splitting the data\n",
        "from sklearn.linear_model import LinearRegression           #supervised machine learning algorithm\n",
        "from sklearn.metrics import r2_score, mean_squared_error    #for checking the accuracy of the model\n",
        "from sklearn.preprocessing import StandardScaler            #for scaling the data\n",
        "import os                                                   #to handle file paths and system-related operations."
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MTKJFP6F-YuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Capstone_project/Regression/SeoulBikeData.csv', encoding= 'unicode_escape')\n",
        "#encoding= 'unicode_escape' to escape non-ASCII characters in a string literal otherwise it was giving an error"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "total_rows, total_columns = df.shape\n",
        "\n",
        "print(f'The dataset has {total_rows} rows and {total_columns} columns')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "total_duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {total_duplicates}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This dataset is very much clean without any duplicates and missing values.** "
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rented Bike Count:** the number of bikes rented in a particular hour\n",
        "\n",
        "**Hour:** the hour of the day (ranging from 0 to 23)\n",
        "\n",
        "**Temperature(°C):** the temperature in degrees Celsius\n",
        "\n",
        "**Humidity(%):** the humidity percentage\n",
        "\n",
        "**Wind speed (m/s):** the wind speed in meters per second\n",
        "\n",
        "**Visibility (10m):** the visibility in 10 meters\n",
        "\n",
        "**Dew point temperature(°C):** the dew point temperature in degrees Celsius\n",
        "\n",
        "**Solar Radiation (MJ/m2):** the amount of solar radiation in mega joules per square meter\n",
        "\n",
        "**Rainfall(mm):** the amount of rainfall in millimeters\n",
        "\n",
        "**Snowfall (cm):** the amount of snowfall in centimeters\n",
        "\n",
        "\n",
        "The summary statistics provide some insights into the distribution and range of values for each variable in the dataset. For example:\n",
        "\n",
        "The mean number of bikes rented per hour is 705, with a standard deviation of 645.\n",
        "\n",
        "The average temperature is 12.9°C, with a standard deviation of 11.9°C.\n",
        "\n",
        "The average humidity is 58%, with a standard deviation of 20%.\n",
        "\n",
        "The highest wind speed recorded is 7.4 m/s.\n",
        "\n",
        "\n",
        "These statistics can be used to gain a better understanding of the dataset and to identify any outliers or unusual values that may need to be further investigated."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before starting Data Wrangling, will first make a copy of our dataset.**"
      ],
      "metadata": {
        "id": "HdLLU3yXQQ2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy of the dataset\n",
        "df_copy=df.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting the Date column which is an object type to datetime format.\n",
        "df_copy['Date'] = pd.to_datetime(df_copy['Date'])\n",
        "df_copy['Date']"
      ],
      "metadata": {
        "id": "T0QHM96aQpsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.info()"
      ],
      "metadata": {
        "id": "ExZYJeIR_tEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As our dataset was clean, not much manipulations were required except changing the data type of Date column from object to datetime."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1\n",
        "**Heatmap:**"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#correlational matrix\n",
        "corr_matrix=df_copy.corr()\n",
        "corr_matrix"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\") #to visually represent the correlation between different variables.\n",
        "\n",
        "# Set the dimensions of the figure\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(10, 5)  # set the dimensions to 10 inches by 5 inches\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3MQrUZBBVfS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation with target variable\n",
        "corr_target = abs(corr_matrix['Rented Bike Count'])\n",
        "corr_target"
      ],
      "metadata": {
        "id": "RaeHPnEzcqg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting highly correlated features\n",
        "strong_corr=corr_target[corr_target>0.25]\n",
        "strong_corr"
      ],
      "metadata": {
        "id": "8IIDQGxWiM-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To vsually represent the correlation between different variables in our dataset"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most significant positive correlation is observed between the Rented Bike Count and the Temperature (0.54) followed by the Dew point temperature (0.38) and Solar Radiation (0.26). This indicates that as the temperature increases, the demand for rented bikes also increases.\n",
        "\n",
        "The most significant negative correlation is observed between the Rented Bike Count and the Humidity (-0.20) followed by Visibility (-0.20) and Wind speed (0.12). This indicates that as the humidity and visibility decrease, the demand for rented bikes increases, while the effect of wind speed is not significant.\n",
        "\n",
        "There is a weak positive correlation between the Rented Bike Count and Hour (0.41), indicating that the time of day has a minor effect on the demand for rented bikes.\n",
        "\n",
        "The Rainfall and Snowfall have a weak negative correlation with the Rented Bike Count (-0.12 and -0.14, respectively), indicating that these weather conditions have a minor effect on the demand for rented bikes.\n",
        "\n",
        "The correlation between the variables of Temperature and Dew point temperature (0.91), Temperature and Solar Radiation (0.35), and Dew point temperature and Solar Radiation (0.09) indicates that these variables are related to each other and may have a combined effect on the demand for rented bikes."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n"
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, the dataset suggests that the temperature and humidity are the most important factors affecting the demand for rented bikes, followed by visibility, solar radiation, and the time of day.\n",
        "From a business point of view, the bike-sharing system could optimize bike availability to match demand patterns and improve customer satisfaction. This could ultimately lead to increased revenue and profitability for the business."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 \n",
        "**Line Plot: Renting of bikes over time**"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Timeline of the data\n",
        "print(f\"First day: {df_copy.Date.min()}; Last day: {df_copy.Date.max()}\")"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unique values of Hour\n",
        "df_copy['Hour'].unique()"
      ],
      "metadata": {
        "id": "P_YTCvbOSBG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualization code for line chart of 'Rented Bike Count' against \"Date\"\n",
        "df_copy.plot(x='Date', y='Rented Bike Count', figsize=(10, 6))\n",
        "plt.ylabel(\"Rented bikes\")\n",
        "plt.xlabel(\"Date\")"
      ],
      "metadata": {
        "id": "rAkCZW0aSPq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualization code for line chart of 'Rented Bike Count' against \"Hour\"\n",
        "df_copy.plot(x='Hour', y='Rented Bike Count', figsize=(10, 6))\n",
        "plt.ylabel(\"Rented bikes\")\n",
        "plt.xlabel(\"Hour\")"
      ],
      "metadata": {
        "id": "xkbG4xnzvP-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the mean values of rented bike count against each hour\n",
        "df_copy.groupby('Hour')['Rented Bike Count'].mean()"
      ],
      "metadata": {
        "id": "6AEtl9DNaZId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the line chart \n",
        "df_copy.groupby('Hour')['Rented Bike Count'].sum().plot(kind='line', figsize=(10, 6))\n",
        "plt.ylabel(\"Rented bikes\")\n",
        "plt.xlabel(\"Hour\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yyVsq6VGXLKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line chart is used to represent the trend of a variable 'rented bike count' over time i.e. 'Date' and 'Hour'. It is useful for visualizing the changes in the value of a variable with respect to time. Line charts can help identify patterns or trends in the data, such as seasonal patterns, upward or downward trendsn. They are particularly useful for displaying continuous data."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the line plot, we can see that the number of rented bikes generally increases over time, with some fluctuations in between. There is some drop in between and again rise and then peak in some which suggests that the demand for rented bikes may be seasonal.\n",
        "Also, this data suggests that there is a high demand for rented bikes during the morning and evening rush hours (7-9am and 5-7pm), which could be due to people commuting to and from work or school. Additionally, there is a relatively low demand for bikes during the early morning hours (12am-5am) and a moderate demand during the rest of the day, with a peak demand during the afternoon hours (2-5pm)."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n"
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, the line plot suggests that there is a seasonal pattern to the demand for rented bikes and the number of rented bikes is high during the peak hours (8am-10am and 4pm-6pm) and low during the early morning hours and late night hours. This information could be useful for bike rental companies to optimize their operations and resource allocation which may be useful for bike rental companies in planning their operations and marketing strategies. Like, ensuring that there are enough bikes available during peak hours and perhaps offering promotions or incentives to encourage off-peak usage."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3\n",
        "**Bar Graphs**"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "#Extracting Year and Day from Date column\n",
        "df_copy[\"year\"] = df_copy.Date.dt.year.astype(int)\n",
        "df_copy[\"day\"] = df_copy.Date.dt.day_name()"
      ],
      "metadata": {
        "id": "V5g_mJh2-uIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.groupby('year')['Rented Bike Count'].sum()"
      ],
      "metadata": {
        "id": "v88wBQe0Zgkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting of years\n",
        "sns.countplot(x=\"year\", data=df_copy)\n",
        "df_copy.year.value_counts()"
      ],
      "metadata": {
        "id": "lQAx89HNB617"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The year 2018 has significantly more occurrences than the year 2017. This could indicate that the dataset predominantly consists of data from the year 2018, and there may be a relatively small amount of data from the year 2017."
      ],
      "metadata": {
        "id": "jRCH25CDHqrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.groupby('day')['Rented Bike Count'].mean().sort_values()"
      ],
      "metadata": {
        "id": "74J2sEWLaQ2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting of days\n",
        "sns.countplot(x=\"day\", data=df_copy)\n",
        "df_copy.day.value_counts()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among the days of the week, Sunday has the highest count of occurrences, followed by Wednesday and Tuesday. Monday has the lowest count of occurrences. This information could be useful in identifying patterns in the dataset based on the day of the week, such as identifying peak and off-peak days, or correlating certain events or external factors with changes in the number of occurrences on different days."
      ],
      "metadata": {
        "id": "YAEcSacqHxv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compute cross-tabulation tables of seasons and years\n",
        "pd.crosstab(df_copy.Seasons,df_copy.year)"
      ],
      "metadata": {
        "id": "m6vHIyJx5XoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting of years and seasons\n",
        "sns.countplot(x=\"Seasons\", hue=\"year\", data=df_copy)"
      ],
      "metadata": {
        "id": "ZsANPPz8HtG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no obervations found in the year 2017 except Winter. Also, the year 2018 has less observations in winter."
      ],
      "metadata": {
        "id": "g6A_pnNfu1jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bikes rented in each Hour\n",
        "df_copy.groupby('Hour')['Rented Bike Count'].sum().sort_values()\n"
      ],
      "metadata": {
        "id": "EgipRuC_yNkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting counts of rented bikes against hours\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Hour',y='Rented Bike Count',data=df_copy)"
      ],
      "metadata": {
        "id": "52QFY01rwF2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bikes are most rented in the morning 8 and in the evening period of 17 to 21."
      ],
      "metadata": {
        "id": "uCmO0cHxGbL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fucntions to put weather columns in certain labels\n",
        "def plot_reting_with_condition(condition, bins, labels):\n",
        "    df_new = df_copy.copy()\n",
        "    df_new[\"new\"] = pd.cut(df[condition], bins, labels=labels)\n",
        "    ax = sns.barplot(data=df_new, x=\"new\", y=\"Rented Bike Count\")\n",
        "    plt.ylabel(\"Sum of rented bikes\")\n",
        "    plt.xlabel(condition)\n",
        "    return ax"
      ],
      "metadata": {
        "id": "goRORSPyICfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting temperature using labels\n",
        "temp_bins = [-20, 0, 10, 20, 30, 40]\n",
        "temp_labels = ['very cold', 'cold', 'mild', 'hot', 'very hot']\n",
        "plot_reting_with_condition('Temperature(°C)', temp_bins, temp_labels)"
      ],
      "metadata": {
        "id": "Vvw3RhhJVK4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting temperature using labels\n",
        "humid_bins = [0, 20, 40, 60, 80, 100]\n",
        "humid_labels = ['very dry', 'dry', 'normal', 'humid', \"very humid\"]\n",
        "plot_reting_with_condition('Humidity(%)', humid_bins, humid_labels)"
      ],
      "metadata": {
        "id": "9USkEtA3V957"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is  to represent categorical data, where each bar represents a category and the height of the bar represents the frequency or count of that category. In the context of bike sharing data, we can use a bar chart to represent the distribution of bike rentals across different time periods, such as hours of the day, days of the week etc."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest number of bikes were rented on Sunday and the lowest on Monday.\n",
        "\n",
        "Also, we can see that all the bikes rented in 2017 were in Winter and none in the other three seasons. In 2018, the bikes were rented in all three seasons except for Autumn.\n",
        "\n",
        "Lastly, number of bikes rented per hour of the day shows that the peak hours for bike rentals were between 5pm and 6pm (17-18 hours) and the lowest was between 4am and 5am (4-5 hours)."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This information can be useful for the bike rental company in terms of managing their inventory, staffing and marketing efforts. For example, they can focus their marketing efforts during the peak hours and months to attract more customers and generate more revenue. They can also adjust their inventory levels and staffing based on the expected demand during different times of the day and year."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4\n",
        "**Histograms**"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will try to explore weather conditions through histograms"
      ],
      "metadata": {
        "id": "PRbuYDqF8Cne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df_copy[\"Rented Bike Count\"])"
      ],
      "metadata": {
        "id": "mVQXY8hyA1tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Temperature\n",
        "sns.histplot(df_copy['Temperature(°C)'])"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.groupby('Temperature(°C)')['Rented Bike Count'].sum().sort_values()"
      ],
      "metadata": {
        "id": "TJx5pHtT_NHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.jointplot(x='Temperature(°C)', y='Rented Bike Count', data=df_copy, kind='hist')\n"
      ],
      "metadata": {
        "id": "ayAvtF5f_6c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Humidity\n",
        "sns.histplot(df_copy['Humidity(%)'])"
      ],
      "metadata": {
        "id": "eWur7PMSA63g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.groupby('Temperature(°C)')['Humidity(%)'].sum().sort_values()"
      ],
      "metadata": {
        "id": "ES5C0kB9CJWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.jointplot(x='Humidity(%)', y='Rented Bike Count', data=df_copy, kind='hist')"
      ],
      "metadata": {
        "id": "4zQhyoKJCYF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the distribution of a single variable. The shape of the histogram gives us an idea of how the data is spread out. For example, if the histogram is bell-shaped, it suggests that the data is normally distributed. If the histogram is skewed to the left or right, it indicates that the data is not symmetric."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The temperature varies from -20 to 40 and the rise in temeperature also gives rise to the counts of rented bikes. So, The graph above suggest that people do not rent bikes when it's very humid or very cold."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n"
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bike sharing company can utilize this information to optimize their bike rental services. They can prioritize the maintenance and availability of bikes during days with favorable weather conditions for bike riding, i.e., moderate temperature and humidity levels. This can help them increase their revenue by catering to the demand during peak periods and reducing operational costs during unfavorable weather conditions when the demand for bikes is expected to be low. Additionally, they can also use this information to plan their marketing campaigns and promotions to attract more customers during favorable weather conditions."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5\n",
        "**Scatter Plots**"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scatter plote for different weather types\n",
        "weather_columns=['Rented Bike Count','Temperature(°C)','Humidity(%)','Wind speed (m/s)','Visibility (10m)','Dew point temperature(°C)','Solar Radiation (MJ/m2)','Rainfall(mm)','Snowfall (cm)']\n",
        "df_weather=df_copy[weather_columns]\n",
        "df_weather.head()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "scatter_matrix(df_weather, figsize=(20, 20))"
      ],
      "metadata": {
        "id": "IYBO4NFBG91N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_weather.corr()"
      ],
      "metadata": {
        "id": "kX_pl93sJlaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the correlations among different weather conditions and counts of rented bikes.In this case, scatter plots were used to visualize the relationship between \"Rented Bike Count\" and the other continuous variables in the dataset. They are used to confirm the correlations observed in the correlation matrix and to visualize the strength and direction of these relationships."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the correlation matrix, we can observe the following:\n",
        "\n",
        "The most positively correlated feature with \"Rented Bike Count\" is \"Temperature (°C)\" with a correlation coefficient of 0.54, which indicates that as the temperature increases, the number of rented bikes also increases.\n",
        "\n",
        "\"Humidity (%)\" has a negative correlation (-0.20) with \"Rented Bike Count\", suggesting that as humidity increases, the number of rented bikes decreases.\n",
        "\n",
        "\"Dew point temperature (°C)\" also has a positive correlation (0.38) with \"Rented Bike Count\", indicating that higher dew point temperature results in a greater number of rented bikes.\n",
        "\n",
        "\"Visibility (10m)\" has a negative correlation (-0.20) with \"Rented Bike Count\", which means that as visibility decreases, the number of rented bikes also decreases.\n",
        "\n",
        "The remaining features, such as \"Wind speed (m/s)\", \"Solar Radiation (MJ/m2)\", \"Rainfall (mm)\", and \"Snowfall (cm)\" have low correlations with \"Rented Bike Count\".\n",
        "\n",
        "Overall, the most important variables in predicting \"Rented Bike Count\" are \"Temperature (°C)\", \"Humidity (%)\", \"Dew point temperature (°C)\", and \"Visibility (10m)\"."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis indicates that temperature has the strongest positive correlation with the number of bikes rented, followed by dew point temperature, solar radiation, and visibility. Humidity, rainfall, and snowfall have weak negative correlations with bike rentals, meaning that as these weather conditions increase, the number of bikes rented tends to decrease slightly. Wind speed has a very weak positive correlation with bike rentals.\n",
        "\n",
        "From a business perspective, this insight suggests that bike rental companies could focus their marketing efforts on days with more favorable weather conditions, such as days with higher temperatures, lower humidity, and greater visibility. They could also consider offering promotions or incentives on days with less favorable weather conditions, such as rainy or snowy days, to encourage more people to rent bikes. Additionally, the analysis could be used to optimize the supply of bikes available for rental based on anticipated demand during specific weather conditions."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5\n",
        "**Box Plots & Normal Distribution**"
      ],
      "metadata": {
        "id": "PihUTQ_YVG79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.boxplot(data=df_weather, orient='v')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yFzDQXRqVRr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating percentage of outliers\n",
        "\n",
        "def calculate_outlier_percentage(column):\n",
        "    q1 = column.quantile(0.25)\n",
        "    q3 = column.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5*iqr\n",
        "    upper_bound = q3 + 1.5*iqr\n",
        "    num_outliers = len(column[(column < lower_bound) | (column > upper_bound)])\n",
        "    outlier_percentage = num_outliers / len(column) * 100\n",
        "    return outlier_percentage\n",
        "\n",
        "for column in df_copy.columns:\n",
        "    if df_copy[column].dtype != 'object':\n",
        "        outlier_percentage = calculate_outlier_percentage(df_copy[column])\n",
        "        print(f\"Percentage of outliers in {column}: {outlier_percentage:.2f}%\")\n"
      ],
      "metadata": {
        "id": "uKIWgd_gZSVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "lGgxlLZ1VG7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplots display the distribution of a dataset and show the median, the interquartile range (IQR), and outliers of the data distribution. Also, to identify potential outliers and to compare the distribution of data between different categories."
      ],
      "metadata": {
        "id": "ZTPVY23AVG7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YmjcnRhFVG7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The percentages of outliers are relatively low for most of the variables, ranging from 0.00% to 1.84%. However, the percentages of outliers for Solar Radiation (7.32%), Rainfall (6.03%), Snowfall (5.06%), and year (8.49%) are higher. This suggests that these columns may require further investigation and preprocessing to address the potential impact of outliers on the data analysis."
      ],
      "metadata": {
        "id": "wUg5Y4dCVG7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "f1NEms9fVG7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying outliers can help businesses understand if there are any extreme values in their data that may be skewing their analysis. For example, the analysis shows that there are outliers in the Solar Radiation, Rainfall, and Snowfall features, indicating that there may be some extreme weather conditions that are not representative of the typical weather patterns in the dataset."
      ],
      "metadata": {
        "id": "_QcN8Wv9K-aV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.head()"
      ],
      "metadata": {
        "id": "0EJ7Y5wNRkDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df_copy.isna().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no missing values in the dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting normal distribution of Wind speed (m/s)\n",
        "plt.figure()\n",
        "sns.distplot(df_copy['Wind speed (m/s)'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zm3s5w4UykA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box-Cox transformation of Wind Speed\n",
        "wind_speed_log = boxcox(df_copy['Wind speed (m/s)'], lmbda=0.5)\n",
        "df_copy['Wind speed (m/s)']=wind_speed_log \n",
        "# Histogram and kernel density estimate\n",
        "plt.figure()\n",
        "sns.distplot(wind_speed_log)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "172qxNYey85g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting normal distribution of Solar Radiation\n",
        "plt.figure()\n",
        "sns.distplot(df_copy['Solar Radiation (MJ/m2)'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IB_DRFhYziCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box-Cox transformation of Solar Radiation\n",
        "solar_rad_log = boxcox(df_copy['Solar Radiation (MJ/m2)'], lmbda=0.5)\n",
        "df_copy['Solar Radiation (MJ/m2)']=solar_rad_log \n",
        "# Histogram and kernel density estimate\n",
        "plt.figure()\n",
        "sns.distplot(solar_rad_log )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B_1eVliHzz0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box-Cox transformation of Rainfall(mm)\n",
        "rainfall_log = boxcox(df_copy['Rainfall(mm)'], lmbda=2)\n",
        "df_copy['Rainfall(mm)']=rainfall_log\n",
        "# Histogram and kernel density estimate\n",
        "plt.figure()\n",
        "sns.distplot(rainfall_log)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "62FpWtE20wQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting normal distribution of Rainfall\n",
        "plt.figure()\n",
        "sns.distplot(df_copy['Rainfall(mm)'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5bO0oNzy0fSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting normal distribution of rented bike count\n",
        "plt.figure()\n",
        "sns.distplot(df_copy['Rented Bike Count'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4BtJAnw0TESQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box-Cox transformation of Rented Bike Count\n",
        "bike_count_log = boxcox(df_copy['Rented Bike Count'], lmbda=0.5)\n",
        "df_copy['Rented Bike Count']=bike_count_log\n",
        "# Histogram and kernel density estimate\n",
        "plt.figure()\n",
        "sns.distplot(bike_count_log)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "133ye-lGxdlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting normal distribution of Snowfall (cm)\n",
        "plt.figure()\n",
        "sns.distplot(df_copy['Snowfall (cm)'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i9_DpoV61hbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box-Cox transformation of Snowfall (cm)\n",
        "snowfall_log = boxcox(df_copy['Snowfall (cm)'], lmbda=2)\n",
        "df_copy['Snowfall (cm)']=snowfall_log\n",
        "# Histogram and kernel density estimate\n",
        "plt.figure()\n",
        "sns.distplot(snowfall_log)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AOFdfZVS1zOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normal distribution of year\n",
        "plt.figure()\n",
        "sns.distplot(df_copy['year'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1D7Pabao25i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating percentage of outliers\n",
        "for column in df_copy.columns:\n",
        "    if df_copy[column].dtype != 'object':\n",
        "        outlier_percentage = calculate_outlier_percentage(df_copy[column])\n",
        "        print(f\"Percentage of outliers in {column}: {outlier_percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "od5rnsCqOTaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Box-Cox transformation method which is used to transform non-normal dependent variables to have a normal distribution. The Box-Cox transformation involves applying a power transformation to the original data. It is particularly useful for data that have a skewed distribution or heteroscedasticity."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking unique values of columns whose dtype is object\n",
        "# select columns with dtype 'object'\n",
        "obj_columns = df_copy.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# loop through the object columns and print unique values\n",
        "for column in obj_columns:\n",
        "    print(column, df_copy[column].unique())"
      ],
      "metadata": {
        "id": "N-L2BrvHOmj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting Holiday and Functioning Day to int through label encoding as its a kind of ordinar categories.\n",
        "df_copy[\"Functioning_Day_int\"]= df_copy[\"Functioning Day\"].map({'Yes': 1, \"No\": 0})\n",
        "df_copy[\"Holiday_int\"] = df_copy.Holiday.map({'No Holiday': 0, \"Holiday\": 1})"
      ],
      "metadata": {
        "id": "Dv7ZqgfdRbqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Droping old Holiday and Functioning Day columns\n",
        "df_copy = df_copy.drop([\"Functioning Day\", \"Holiday\"], axis=1)"
      ],
      "metadata": {
        "id": "pzTpM4ZdR8L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One hot encoding for remaining object columns Seasons and Day\n",
        "df_categorical=pd.get_dummies(df_copy.select_dtypes(include='object'))"
      ],
      "metadata": {
        "id": "0skiONwaTCNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet is performing label encoding on the 'Functioning Day' and 'Holiday' columns of the DataFrame. Label encoding is a type of categorical encoding that assigns each unique category in a column a numerical label, which is then used to represent the category in the data. Here,label encoding is useful because categorical data has a clear and natural ordering.\n",
        "While in the case of the categorical columns 'Seasons' and 'day', they are converted into binary vectors using one hot encoding.In one hot encoding, each category in a categorical variable is converted into a binary vector of 0's and 1's, where 1 represents the presence of the category and 0 represents the absence. This is done to represent each category as a separate feature/column in the data, with a value of 1 indicating the presence of that category and 0 indicating the absence."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In time series analysis, seasonal patterns are common and repeating trends that occur within a year. For example, we may expect more bike rentals during summer and fewer bike rentals during winter. Therefore, it can be helpful to incorporate seasonality into our analysis to improve the accuracy of our predictions.\n",
        "\n",
        "The cos_month and sin_month columns are created using a technique called \"cyclical encoding\". This technique is used to encode cyclical or periodic features like months, days of the week, and hours of the day, into a continuous numerical format.\n",
        "\n",
        "Instead of simply encoding the month as an integer value (e.g., January = 1, February = 2, etc.), we convert the month value to an angle between 0 and 2π radians, and then encode it using sine and cosine functions. This allows us to represent the cyclical nature of months in a continuous, smooth way."
      ],
      "metadata": {
        "id": "3MCALT7SK6k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding month to the dataset\n",
        "df_copy[\"month\"] = df_copy.Date.dt.month"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy['cos_month'] = np.cos((df_copy.month) / 12 * 2 * np.pi)\n",
        "df_copy['sin_month'] = np.sin((df_copy.month) / 12 * 2 * np.pi)"
      ],
      "metadata": {
        "id": "qk02PwFaLHm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy['cos_hour'] = np.cos((df_copy.Hour) / 24 * 2 * np.pi)\n",
        "df_copy['sin_hour'] = np.sin((df_copy.Hour) / 24 * 2 * np.pi)"
      ],
      "metadata": {
        "id": "wnNf0AI0LaTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#droping old columns of Date, Hour, Month\n",
        "df_copy= df_copy.drop([\"Date\", \"Hour\", \"month\"], axis=1)"
      ],
      "metadata": {
        "id": "bTfIosTMLxP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking dtypes of each column\n",
        "df_copy.dtypes"
      ],
      "metadata": {
        "id": "k7VXi2AvOS4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.year.unique()"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are only 2017 and 2018 values we can convert it to 0 or 1."
      ],
      "metadata": {
        "id": "tkYVaZJOd5zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy['year'] = df_copy['year'].apply(lambda x: 0 if x == 2017 else 1)"
      ],
      "metadata": {
        "id": "gz5r0rh6dxvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extrancting numerical columns\n",
        "df_numerical=df_copy.select_dtypes(exclude='object')"
      ],
      "metadata": {
        "id": "tRCmLnlYTyAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concating the categorical and numerical columns \n",
        "df_model=pd.concat([df_categorical,df_numerical],axis=1)"
      ],
      "metadata": {
        "id": "PSkA8TR4UpZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the range of values across features\n",
        "print('Range of values across features:')\n",
        "print(df_model.max() - df_model.min())"
      ],
      "metadata": {
        "id": "BtGKlKtSgYYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit the scaler on the data\n",
        "scaler.fit(df_model)\n",
        "\n",
        "# transform the data using the scaler\n",
        "df_model= pd.DataFrame(scaler.transform(df_model), columns=df_model.columns)"
      ],
      "metadata": {
        "id": "uVN-jAglrb0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_model.head()"
      ],
      "metadata": {
        "id": "JUmwaH-xVdIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason for doing data scaling using Standard Scaler is to ensure that all features are on a comparable scale and to avoid some features dominating others due to their larger magnitude. This can lead to more stable and accurate model performance, especially in cases where some features have a much larger range of values than others."
      ],
      "metadata": {
        "id": "WhtvF3Tjxmzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=4)\n",
        "for train_index, test_index in splitter.split(df_model, df_model.year):\n",
        "    train_set = df_model[df_model.index.isin(train_index)]\n",
        "    test_set = df_model[df_model.index.isin(test_index)]"
      ],
      "metadata": {
        "id": "O7In99NjbDkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.shape"
      ],
      "metadata": {
        "id": "v1GId73vbkkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set.shape"
      ],
      "metadata": {
        "id": "7vMUHzXHb1VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train_set['Rented Bike Count']\n",
        "X_train = train_set.drop('Rented Bike Count', axis=1)\n",
        "y_test = test_set['Rented Bike Count']\n",
        "X_test = test_set.drop('Rented Bike Count', axis=1)\n"
      ],
      "metadata": {
        "id": "PXDHjZNaftHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we know, the obervations in the year columns had very much differnce because the year 2017 had only observations for winter season, Therefore, value counts are less for the year 2017 and so we need to have stratified splitting of the data set. Here, test size is chosen to be 20% because its a common practice to use a test size of around 20% of the data, which means that 80% of the data is used for training and 20% for testing."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6 ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Linear Regression model\n",
        "lr_model = LinearRegression()\n",
        "\n",
        "# Fit the model to the training data\n",
        "lin_reg_model=lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = lin_reg_model.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#accuracy score\n",
        "# calculate the mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "# calculate the R^2 score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R^2 Score:\", r2)"
      ],
      "metadata": {
        "id": "TrqTlBOKyRew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(y_test, y_pred)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='red')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZswWqXci3Rtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is used to create a scatter plot of the actual target values (y_test) versus the predicted target values (y_pred) from a regression model.\n",
        "\n",
        "The plot will have points for each instance in the test set, where the x-coordinate is the actual target value and the y-coordinate is the predicted target value.\n",
        "\n",
        "The plot will also have a diagonal line (in red, dashed) that represents the perfect prediction line where the predicted target value is equal to the actual target value.\n",
        "\n",
        "By comparing the scatter plot to this perfect prediction line, we can visually assess the performance of the regression model. If most points fall close to the perfect prediction line, it indicates that the model has a good predictive power. If the points are widely spread, it indicates that the model is not doing well in making accurate predictions."
      ],
      "metadata": {
        "id": "kG-ZQp604PUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model used is linear regression, which is a simple and commonly used machine learning algorithm for predicting continuous values. The R^2 score for this model is 0.653, which means that the model explains 65.3% of the variability in the target variable. This is a moderate level of performance, indicating that the model is able to capture some of the patterns in the data but there is still room for improvement.\n",
        "\n",
        "The mean squared error (MSE) for this model is 0.346, which is a measure of how close the predicted values are to the true values. A lower MSE indicates better performance, and in this case, the MSE suggests that the model's predictions are fairly close to the true values, with an average squared error of 0.346.\n",
        "\n"
      ],
      "metadata": {
        "id": "AldmF3z-3ChJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#feature importance\n",
        "lin_reg_model.coef_"
      ],
      "metadata": {
        "id": "xgOqJjntPnCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Instantiate\n",
        "rf_mod = RandomForestRegressor(max_depth=2, random_state=123, \n",
        "              n_estimators=100, oob_score=True)\n",
        "\n",
        "# Fit\n",
        "rf_mod.fit(X_train, y_train)\n",
        "\n",
        "# Print\n",
        "print(X_train.columns)\n",
        "print(rf_mod.feature_importances_)"
      ],
      "metadata": {
        "id": "Vmlrvy8AWgYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code was used to train a random forest regression model and to check the feature importance of each predictor variable in the model. Looking at the feature importances printed by the code, it seems that the most important feature in the Random Forest model is the \"Temperature(°C)\" feature, with a feature importance of 0.676. The next most important features are \"Humidity(%)\", \"cos_month\", \"Functioning_Day_int\", and \"Holiday_int\"."
      ],
      "metadata": {
        "id": "HycVVn164p9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(mean_squared_error(y_true=y_test, y_pred=rf_mod.predict(X_test)))\n",
        "print(r2_score(y_true=y_test, y_pred=rf_mod.predict(X_test)))"
      ],
      "metadata": {
        "id": "6FAV_ChG1Ff8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the MSE is 0.529 and the R-squared score is 0.469, which suggests that the model is not performing well and there is a lot of variance that is not being explained by the model."
      ],
      "metadata": {
        "id": "Ho_CSO7E5yKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lasso Regularization\n",
        "# Import modules\n",
        "from sklearn.linear_model import Lasso, LassoCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Instantiate cross-validated lasso, fit\n",
        "lasso_cv = LassoCV(alphas=None, cv=10, max_iter=10000)\n",
        "lasso_cv.fit(X_train, y_train)\n",
        "\n",
        "# Instantiate lasso, fit, predict and print MSE\n",
        "lasso = Lasso(alpha = lasso_cv.alpha_)\n",
        "lasso.fit(X_train, y_train)\n",
        "print(mean_squared_error(y_true=y_test, y_pred=lasso.predict(X_test)))\n",
        "print(r2_score(y_true=y_test, y_pred=lasso.predict(X_test)))"
      ],
      "metadata": {
        "id": "Jw_qh4TFfdbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ridge Regression\n",
        "# Import modules\n",
        "from sklearn.linear_model import Ridge, RidgeCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Instantiate cross-validated ridge, fit\n",
        "ridge_cv = RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
        "ridge_cv.fit(X_train, y_train)\n",
        "\n",
        "# Instantiate ridge, fit, predict and print MSE\n",
        "ridge = Ridge(alpha = ridge_cv.alpha_)\n",
        "ridge.fit(X_train, y_train)\n",
        "print(mean_squared_error(y_true=y_test, y_pred=ridge.predict(X_test)))\n",
        "print(r2_score(y_true=y_test, y_pred=lasso.predict(X_test)))"
      ],
      "metadata": {
        "id": "rTJ3C-mngaDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "# Create an instance of the Lasso regression model\n",
        "lasso_mod = Lasso()\n",
        "\n",
        "# Create an instance of the GridSearchCV object\n",
        "grid_search = GridSearchCV(lasso_mod, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Use the best hyperparameters to predict on the test set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model using mean squared error and R^2\n",
        "print('MSE:', mean_squared_error(y_test, y_pred))\n",
        "print('R^2:', r2_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "iNif_A8v7yVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is used to perform hyperparameter tuning by searching over a grid of parameter values for a given estimator. It exhaustively tries all combinations of hyperparameters defined in the grid and returns the combination of hyperparameters that gives the best performance as evaluated by cross-validation. By using GridSearchCV, we can automate the process of hyperparameter tuning and find the optimal values for hyperparameters without having to manually try different combinations. "
      ],
      "metadata": {
        "id": "qSITA5ZA8W3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, an improvement of 0.01 in R^2. "
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "import time\n",
        "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer, r2_score\n",
        "\n",
        "mse_scorer = make_scorer(mean_squared_error)\n",
        "mae_scorer = make_scorer(mean_absolute_error)\n",
        "r2_scorer = make_scorer(r2_score)"
      ],
      "metadata": {
        "id": "ErGKsPTqwSGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBRegressor(n_estimators=500, max_depth=3, colsample_bytree=1)\n",
        "\n",
        "xgb.fit(X_train.values, y_train.values)"
      ],
      "metadata": {
        "id": "S0ALht4MRi-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = pd.DataFrame(index=X_train.columns, data=xgb.feature_importances_)\n",
        "\n",
        "feature_importance = feature_importance.sort_values(0, ascending=False)\n",
        "\n",
        "feature_importance"
      ],
      "metadata": {
        "id": "75ENmxGnS0zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_real, y_pred, metric):\n",
        "    return metric(y_real, y_pred)\n",
        "\n",
        "def model_evaluate(model, X_train, y_train, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    metrics = {}\n",
        "    #RMSE Test\n",
        "    rmse_test = np.sqrt(calculate_metrics(y_test, y_pred, mean_squared_error))\n",
        "    #RMSE Train\n",
        "    rmse_train = np.sqrt(calculate_metrics(y_train, y_pred_train, mean_squared_error))\n",
        "    r2_test = calculate_metrics(y_test, y_pred, r2_score)\n",
        "    r2_train = calculate_metrics(y_train, y_pred_train, r2_score)\n",
        "    metrics = {\n",
        "              'RMSE Test': rmse_test,\n",
        "              'RMSE Train': rmse_train,\n",
        "              'r2 Test': r2_test,\n",
        "              'r2 Train': r2_train}\n",
        "\n",
        "    return metrics "
      ],
      "metadata": {
        "id": "ICL6O3aNVIhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.columns.shape\n",
        "X_train.columns.shape"
      ],
      "metadata": {
        "id": "TpRCFiPXWBIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#index_correct = X_test[X_test.Functioning_Day_int == 0].index\n",
        "\n",
        "raw_preds = xgb.predict(X_test.values)\n",
        "\n",
        "test_predictions = pd.DataFrame(np.array([X_test.index, raw_preds, y_test]).T, columns= ['index', 'raw_preds', 'real value'])\n",
        "test_predictions = test_predictions.set_index(\"index\")\n",
        "test_predictions"
      ],
      "metadata": {
        "id": "_90mvUaPW5PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_absolute_error(y_test, test_predictions.raw_preds.values)"
      ],
      "metadata": {
        "id": "N3EdbPeEXe_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pred = xgb.predict(X_train.values)\n",
        "mean_squared_error(y_train, train_pred)"
      ],
      "metadata": {
        "id": "R1GgQdNkXsXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train r2 Score:\")\n",
        "r2_score(y_train, train_pred)"
      ],
      "metadata": {
        "id": "YYF4EV5sXyC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test r2 Score:\")\n",
        "r2_score(y_test, raw_preds)"
      ],
      "metadata": {
        "id": "3nF5_FRyX38R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions[test_predictions.raw_preds < 0]"
      ],
      "metadata": {
        "id": "fG7MiTgGYD9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we know the target variable cannot have negative values, so any negative predicted values are replaced with 0 to ensure the predictions are valid."
      ],
      "metadata": {
        "id": "jLlWVz-gDJk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#applies a condition where if the predicted value is less than 0, it replaces that value with 0, otherwise, it keeps the predicted value as is. \n",
        "raw_preds2 = np.where(raw_preds<0, 0, raw_preds)\n",
        "r2_score(y_test, raw_preds2)"
      ],
      "metadata": {
        "id": "E7Vgf2xQYdef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The XGBRegressor model is a popular choice for regression tasks when dealing with structured data. It is particularly useful when dealing with high-dimensional datasets, as it is designed to handle large numbers of features. Additionally, XGBoost is known for its fast training speed and high accuracy. In the context of this specific model, the r2 score of 0.96 for the training set and 0.92 for the testing set suggests that the model is a good fit for the data and performs well in predicting the target variable. However, the r2 score of 0.49 for the modified predicted values indicates that the model's predictions may not be very accurate when negative values are present in the predicted values."
      ],
      "metadata": {
        "id": "4-a0C6QYad7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, make_scorer\n",
        "\n",
        "# define your KNeighborsRegressor model\n",
        "knn = KNeighborsRegressor()\n",
        "\n",
        "# define the hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'p': [1, 2, 3],\n",
        "}\n",
        "\n",
        "# create a GridSearchCV object with your KNeighborsRegressor model and hyperparameter grid\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring=make_scorer(mean_squared_error))\n",
        "\n",
        "# fit the GridSearchCV object to your training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# get the best model from GridSearchCV\n",
        "best_knn = grid_search.best_estimator_\n",
        "\n",
        "# print the best parameters and best score from GridSearchCV\n",
        "print(\"Best Parameters: \", grid_search.best_params_)\n",
        "print(\"Best Score: \", -grid_search.best_score_)  # take the negative of mean squared error to get the best score\n",
        "\n",
        "# evaluate the best model on your test data\n",
        "test_preds = best_knn.predict(X_test)\n",
        "test_rmse = mean_squared_error(y_test, test_preds, squared=False)\n",
        "test_r2 = best_knn.score(X_test, y_test)\n",
        "print(\"Test RMSE:\", test_rmse)\n",
        "print(\"Test R^2:\", test_r2)"
      ],
      "metadata": {
        "id": "WBu2mR0ibYqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning with GridSearchCV for K-Nearest Neighbors Regressor can help to identify the optimal set of hyperparameters for the model that provides the best performance on the given dataset. K-Nearest Neighbors Regressor is a non-parametric model that relies heavily on the value of its hyperparameters. Tuning the hyperparameters such as the number of neighbors (k), distance metric, and weights of the neighbors can help to improve the performance of the KNN Regressor model. GridSearchCV can systematically search through a range of hyperparameters to find the optimal combination of hyperparameters that provide the best performance on the given dataset, without the need for manual trial and error."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of the XGBoost model, the train R2 score is 0.96 and the test R2 score is 0.92, which indicates that the model is fitting the data well and is able to generalize to new data.\n",
        "\n",
        "In the case of the KNN model, the test R2 score is 0.87, which is also a good score, indicating that the model is performing well on the test data."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Instantiate the regressor\n",
        "regr = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=0)\n",
        "\n",
        "# Fit the model\n",
        "regr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = regr.predict(X_test)\n",
        "\n",
        "# Print accuracy score\n",
        "print(mean_squared_error(y_test, y_pred))\n",
        "print(r2_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "0x0JZ93lKOKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BaggingRegressor ensemble model with DecisionTreeRegressor is a good choice when you have a complex regression problem with high variance and want to reduce overfitting. The DecisionTreeRegressor is a high-variance, low-bias model that is prone to overfitting. By combining multiple DecisionTreeRegressor models through bagging, you can reduce the variance and improve the generalization performance of the model.\n",
        "In this case, the test MSE is 0.083 and the test R2 score is 0.916, which indicates that the model is performing well."
      ],
      "metadata": {
        "id": "UkLQ1PCVZwFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# define your XGBRegressor model\n",
        "xgb = XGBRegressor()\n",
        "\n",
        "# define the hyperparameter grid for RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.5],\n",
        "    'colsample_bytree': [0.5, 0.7, 0.9],\n",
        "    'subsample': [0.5, 0.7, 0.9],\n",
        "}\n",
        "\n",
        "# create a RandomizedSearchCV object with your XGBRegressor model and hyperparameter grid\n",
        "random_search = RandomizedSearchCV(xgb, param_grid, n_iter=10, cv=5, random_state=42)\n",
        "\n",
        "# fit the RandomizedSearchCV object to your training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# get the best model from RandomizedSearchCV\n",
        "best_xgb = random_search.best_estimator_\n",
        "\n",
        "# print the best parameters and best score from RandomizedSearchCV\n",
        "print(\"Best Parameters: \", random_search.best_params_)\n",
        "print(\"Best Score: \", random_search.best_score_)\n",
        "\n",
        "# evaluate the best model on your test data\n",
        "test_metrics = model_evaluate(best_xgb, X_train, y_train, X_test, y_test)\n",
        "print(test_metrics)\n",
        "\n"
      ],
      "metadata": {
        "id": "Z2D5wssqInQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the hyperparameter space is large, and trying all possible combinations would take a very long time. In such cases, RandomizedSearchCV samples a subset of the hyperparameter space, allowing for a more efficient search. It is also useful when the hyperparameters have varying levels of importance."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the score has improved by approximately 3%"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work .***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Future Work:\n",
        "\n",
        "Include more relevant features, such as weather conditions and events in the city.\n",
        "Explore other types of machine learning models, such as neural networks.\n",
        "Optimize the models by tuning their hyperparameters using grid search or other optimization techniques.\n",
        "Develop a real-time prediction system that can update predictions as new data becomes available.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "The dataset used may not be representative of all bike sharing systems.\n",
        "External factors, such as traffic congestion and events, were not included in the analysis.\n",
        "Additional evaluation metrics and economic impacts were not considered.\n",
        "Limitations of the models used include linear assumptions and overfitting potential."
      ],
      "metadata": {
        "id": "mg5VvNYwcsMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the various outputs provided for this ML project, it can be concluded that different machine learning models have been used and evaluated based on their performance metrics such as Root Mean Squared Error (RMSE) and R-squared (R2) scores.\n",
        "\n",
        "Model 1: Linear Regression Model\n",
        "\n",
        "The Linear Regression model achieved an RMSE of 0.367 and an R2 score of 0.865 on the test data.\n",
        "\n",
        "Model 2: K-Nearest Neighbors Model\n",
        "\n",
        "After hyperparameter tuning, the K-Nearest Neighbors model achieved an RMSE of 0.365 and an R2 score of 0.866 on the test data.\n",
        "\n",
        "Model 3: Bagging Regressor Model\n",
        "\n",
        "The Bagging Regressor model achieved an RMSE of 0.083 and an R2 score of 0.916 on the test data.\n",
        "\n",
        "Model 4: XGBoost Regressor Model\n",
        "\n",
        "After hyperparameter tuning, the XGBoost Regressor model achieved an RMSE of 0.232 and an R2 score of 0.946 on the test data.\n",
        "\n",
        "From the above outputs, it can be concluded that the XGBoost Regressor model performed the best among the four models as it achieved the lowest RMSE value and the highest R2 score. Additionally, hyperparameter tuning improved the performance of both the K-Nearest Neighbors model and the XGBoost Regressor model. Therefore, hyperparameter tuning is a useful technique to improve the performance of machine learning model.\n",
        "\n",
        "The analysis of the bike sharing data has revealed some important insights into the factors that influence the demand for rented bikes. The most significant positive correlation was observed between the Rented Bike Count and the Temperature, followed by Dew point temperature and Solar Radiation. The most significant negative correlation was observed between the Rented Bike Count and the Humidity, followed by Visibility and Wind speed. There was a weak positive correlation between the Rented Bike Count and Hour, indicating that the time of day has a minor effect on the demand for rented bikes. Rainfall and Snowfall had a weak negative correlation with the Rented Bike Count, indicating that these weather conditions have a minor effect on the demand for rented bikes. The dataset suggests that temperature and humidity are the most important factors affecting the demand for rented bikes, followed by visibility, solar radiation, and the time of day.\n",
        "\n",
        "The line plot of the data showed a seasonal pattern to the demand for rented bikes, with the number of rented bikes being high during the peak hours of the day and low during the early morning and late night hours. This information could be useful for bike rental companies to optimize their operations and resource allocation.\n",
        "\n",
        "The bar chart revealed that the highest number of bikes were rented on Sunday and the lowest on Monday. Also, all the bikes rented in 2017 were in Winter and none in the other three seasons. In 2018, the bikes were rented in all three seasons except for Autumn. The number of bikes rented per hour of the day showed that the peak hours for bike rentals were between 5 pm and 6 pm and the lowest was between 4 am and 5 am.\n",
        "\n",
        "The histogram of the temperature data showed that people do not rent bikes when it's very humid or very cold. The bike-sharing company can utilize this information to optimize their bike rental services, prioritize maintenance and availability of bikes during days with favorable weather conditions, and plan their marketing campaigns and promotions to attract more customers during favorable weather conditions.\n",
        "\n",
        "In conclusion, the bike-sharing company can use the insights from the analysis to optimize their bike rental services, prioritize maintenance and availability of bikes during peak periods, and plan their marketing campaigns and promotions to attract more customers during favorable weather conditions."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}